{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "automated_essay_scoring.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNDRRCZFUULhCp8OYFDE6mI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Gail529/AES/blob/main/automated_essay_scoring.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BTILa78Y3xVM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5baa2c35-3af5-4fe6-82cd-ed2f79d3ed4c"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import sent_tokenize\n",
        " \n",
        "import re\n",
        "import string\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")  \n",
        "from gensim.models import Word2Vec\n",
        " \n",
        " \n",
        "#essay preprocessing \n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "import string\n",
        "import re\n",
        "from nltk.tokenize import word_tokenize,sent_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "#training the neural network \n",
        "from tensorflow.keras.layers import Embedding\n",
        "from keras.models import Sequential, load_model, model_from_config\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Lambda\n",
        "from tensorflow.keras.layers import Flatten\n",
        "import keras.backend as K"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "1_-bDXNbIUpe",
        "outputId": "4d76d707-c50a-4efc-aa23-d62bfe4e8d93"
      },
      "source": [
        "essay_data= pd.read_excel('/content/training_set_rel3.xls',usecols=['essay_id','essay','domain1_score'])\n",
        "essay_data.head(2)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>essay_id</th>\n",
              "      <th>essay</th>\n",
              "      <th>domain1_score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>Dear local newspaper, I think effects computer...</td>\n",
              "      <td>8.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>Dear @CAPS1 @CAPS2, I believe that using compu...</td>\n",
              "      <td>9.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   essay_id                                              essay  domain1_score\n",
              "0         1  Dear local newspaper, I think effects computer...            8.0\n",
              "1         2  Dear @CAPS1 @CAPS2, I believe that using compu...            9.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xpkl7yetIdWD",
        "outputId": "2b3338c8-a043-4cee-e237-c6b57609c596"
      },
      "source": [
        "x=essay_data['essay']\n",
        "y=essay_data['domain1_score']\n",
        "len(x) "
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "12978"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5blTB1-sJ2Mc",
        "outputId": "2a0b6f8c-caf7-47b1-9614-272e66efef9d"
      },
      "source": [
        "essay_1=x[0]\n",
        "print(essay_1)\n",
        "#clean a single essay\n",
        "print(sent_tokenize(essay_1))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dear local newspaper, I think effects computers have on people are great learning skills/affects because they give us time to chat with friends/new people, helps us learn about the globe(astronomy) and keeps us out of troble! Thing about! Dont you think so? How would you feel if your teenager is always on the phone with friends! Do you ever time to chat with your friends or buisness partner about things. Well now - there's a new way to chat the computer, theirs plenty of sites on the internet to do so: @ORGANIZATION1, @ORGANIZATION2, @CAPS1, facebook, myspace ect. Just think now while your setting up meeting with your boss on the computer, your teenager is having fun on the phone not rushing to get off cause you want to use it. How did you learn about other countrys/states outside of yours? Well I have by computer/internet, it's a new way to learn about what going on in our time! You might think your child spends a lot of time on the computer, but ask them so question about the economy, sea floor spreading or even about the @DATE1's you'll be surprise at how much he/she knows. Believe it or not the computer is much interesting then in class all day reading out of books. If your child is home on your computer or at a local library, it's better than being out with friends being fresh, or being perpressured to doing something they know isnt right. You might not know where your child is, @CAPS2 forbidde in a hospital bed because of a drive-by. Rather than your child on the computer learning, chatting or just playing games, safe and sound in your home or community place. Now I hope you have reached a point to understand and agree with me, because computers can have great effects on you or child because it gives us time to chat with friends/new people, helps us learn about the globe and believe or not keeps us out of troble. Thank you for listening.\n",
            "[['dear', 'local', 'newspaper', 'think', 'effects', 'computers', 'people', 'great', 'learning', 'skills', 'affects', 'give', 'us', 'time', 'chat', 'friends', 'new', 'people', 'helps', 'us', 'learn', 'globe', 'astronomy', 'keeps', 'us', 'troble'], ['thing'], ['dont', 'think'], ['would', 'feel', 'teenager', 'always', 'phone', 'friends'], ['ever', 'time', 'chat', 'friends', 'buisness', 'partner', 'things'], ['well', 'new', 'way', 'chat', 'computer', 'plenty', 'sites', 'internet', 'organization1', 'organization2', 'caps1', 'facebook', 'myspace', 'ect'], ['think', 'setting', 'meeting', 'boss', 'computer', 'teenager', 'fun', 'phone', 'rushing', 'get', 'cause', 'want', 'use'], ['learn', 'countrys', 'states', 'outside'], ['well', 'computer', 'internet', 'new', 'way', 'learn', 'going', 'time'], ['might', 'think', 'child', 'spends', 'lot', 'time', 'computer', 'ask', 'question', 'economy', 'sea', 'floor', 'spreading', 'even', 'date1', 'surprise', 'much', 'knows'], ['believe', 'computer', 'much', 'interesting', 'class', 'day', 'reading', 'books'], ['child', 'home', 'computer', 'local', 'library', 'better', 'friends', 'fresh', 'perpressured', 'something', 'know', 'isnt', 'right'], ['might', 'know', 'child', 'caps2', 'forbidde', 'hospital', 'bed', 'drive'], ['rather', 'child', 'computer', 'learning', 'chatting', 'playing', 'games', 'safe', 'sound', 'home', 'community', 'place'], ['hope', 'reached', 'point', 'understand', 'agree', 'computers', 'great', 'effects', 'child', 'gives', 'us', 'time', 'chat', 'friends', 'new', 'people', 'helps', 'us', 'learn', 'globe', 'believe', 'keeps', 'us', 'troble'], ['thank', 'listening']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x-EzcAo4URAF"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gM5Xp0rbEaC_"
      },
      "source": [
        "def sent_tokenize(text):\n",
        "    stripped_essay = text.strip()\n",
        "    \n",
        "    tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
        "    raw_sentences = tokenizer.tokenize(stripped_essay)\n",
        "    \n",
        "    tokenized_sentences = []\n",
        "    for raw_sentence in raw_sentences:\n",
        "        clean_sentence = re.sub(\"[^a-zA-Z0-9]\",\" \", raw_sentence)\n",
        "        words = clean_sentence.lower().split()\n",
        "        stops = set(stopwords.words(\"english\"))\n",
        "        words = [w for w in words if not w in stops]\n",
        "        tokenized_sentences.append(words)\n",
        "\n",
        "    return tokenized_sentences\n"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "es33b9LNUUla"
      },
      "source": [
        "cleaned_essays=[]\n",
        "for i in range(0,len(x)):\n",
        "    essay=x[i]\n",
        "    cleaned_essays.append(sent_tokenize(essay))\n",
        "    "
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1r4VzuF7OfKM"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l7UIo0SnAWma"
      },
      "source": [
        "sentences=[]\n",
        "for essay in x:\n",
        "    sentences += sent_tokenize(essay)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-rKwV3LzB1Mt"
      },
      "source": [
        "sentences[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pt1qTaNQ_Dva"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rvMn_Q2JLeTd",
        "outputId": "4409f026-90d1-4d6c-b511-9f9181369c2d"
      },
      "source": [
        "text_dim = 300\n",
        "print(\"Training Word2Vec model...\")\n",
        "wordvec_model = Word2Vec(sentences, size=text_dim, window=5, min_count=3, workers=4, sg=1)\n",
        "print(\"Word2Vec model created.\")\n",
        "print(\"%d unique words represented by %d dimensional vectors\" % (len(wordvec_model.wv.vocab), text_dim))\n",
        "wordvec_model.save('wordvec_model')\n",
        "print(\"Word2Vec model saved.\")"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training Word2Vec model...\n",
            "Word2Vec model created.\n",
            "13134 unique words represented by 300 dimensional vectors\n",
            "Word2Vec model saved.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jIwuz6r8PgLc"
      },
      "source": [
        "def create_average_vec(essay):\n",
        "    average = np.zeros((text_dim,), dtype='float32')\n",
        "    num_words = 0.\n",
        "    for word in essay.split():\n",
        "        if word in wordvec_model.wv.vocab:\n",
        "            average = np.add(average, wordvec_model.wv[word])\n",
        "            num_words += 1.\n",
        "    if num_words != 0.:\n",
        "        average = np.divide(average, num_words)\n",
        "    return average"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nBIOCDifPoMk"
      },
      "source": [
        "cleaned_vec = np.zeros((training_set.shape[0], text_dim), dtype=\"float32\")  \n",
        "for i in range(len(train_cleaned)):\n",
        "    cleaned_vec[i] = create_average_vec(train_cleaned[i])\n",
        "\n",
        "print(\"Word vectors for all essays in the training data set are of shape:\", cleaned_vec.shape)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7gaRkCN1TBy-"
      },
      "source": [
        "def clean_essays(essay):\n",
        "    clean_essay= re.sub(\"[^a-zA-Z]\", \" \",essay)\n",
        "    words = essay.lower().split()\n",
        "    stops = set(stopwords.words(\"english\"))\n",
        "    words = [w for w in words if not w in stops]\n",
        "    return (words)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UmRZvEdiSh6s"
      },
      "source": [
        "clean_test_essays = []\n",
        "for essay in x:\n",
        "        clean_test_essays.append(clean_essays( essay))"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Tt05pTQUQOA"
      },
      "source": [
        "clean_test_essays[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xu1y2a6ePxN0"
      },
      "source": [
        "def makeFeatureVec(words, model, num_features):\n",
        "    \"\"\"Make Feature Vector from the words list of an Essay.\"\"\"\n",
        "    featureVec = np.zeros((num_features,),dtype=\"float32\")\n",
        "    num_words = 0.\n",
        "    index2word_set = set(model.wv.index2word)\n",
        "    for word in words:\n",
        "        if word in index2word_set:\n",
        "            num_words += 1\n",
        "            featureVec = np.add(featureVec,model[word])        \n",
        "    featureVec = np.divide(featureVec,num_words)\n",
        "    return featureVec\n",
        "\n",
        "def getAvgFeatureVecs(essays, model, num_features):\n",
        "    \"\"\"Main function to generate the word vectors for word2vec model.\"\"\"\n",
        "    counter = 0\n",
        "    essayFeatureVecs = np.zeros((len(essays),num_features),dtype=\"float32\")\n",
        "    for essay in essays:\n",
        "        essayFeatureVecs[counter] = makeFeatureVec(essay, model, num_features)\n",
        "        counter = counter + 1\n",
        "    return essayFeatureVecs"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GZQHiY8IWgyS"
      },
      "source": [
        "essay_Vecs = getAvgFeatureVecs(clean_test_essays, wordvec_model, text_dim)\n"
      ],
      "execution_count": 32,
      "outputs": []
    }
  ]
}